{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wordcloud as wc\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo de Twitter: ver [vínculo](https://towardsdatascience.com/tweepy-for-beginners-24baf21f2c25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "class TweetMiner(object):\n",
    "\n",
    "    result_limit    =   20    \n",
    "    data            =   []\n",
    "    api             =   False\n",
    "\n",
    "    twitter_keys = {\n",
    "        'consumer_key':        '---YOUR-KEY---',\n",
    "        'consumer_secret':     '---YOUR-KEY---',\n",
    "        'access_token_key':    '---YOUR-KEY---',\n",
    "        'access_token_secret': '---YOUR-KEY---'\n",
    "    }\n",
    "    \n",
    "    tweet_mode = 'extended'\n",
    "\n",
    "    def __init__(self, keys_dict=twitter_keys, api=api, result_limit = 20):\n",
    "        self.twitter_keys = keys_dict\n",
    "        auth = tweepy.OAuthHandler(keys_dict['consumer_key'], keys_dict['consumer_secret'])\n",
    "        auth.set_access_token(keys_dict['access_token_key'], keys_dict['access_token_secret'])\n",
    "        self.api = tweepy.API(auth)\n",
    "        self.twitter_keys = keys_dict\n",
    "        self.result_limit = result_limit\n",
    "        \n",
    "\n",
    "    def mine_user_tweets(\n",
    "        self, user=\"nytimes\",\n",
    "        mine_rewteets=False,\n",
    "        max_pages=20\n",
    "    ):\n",
    "        data           =  []\n",
    "        last_tweet_id  =  False\n",
    "        page           =  1\n",
    "        while page <= max_pages:\n",
    "            if last_tweet_id:\n",
    "                statuses   =   self.api.user_timeline(screen_name=user,\n",
    "                    count=self.result_limit,\n",
    "                    max_id=last_tweet_id - 1,\n",
    "                    tweet_mode = 'extended',\n",
    "                    include_retweets=mine_rewteets\n",
    "                )        \n",
    "            else:\n",
    "                statuses   =   self.api.user_timeline(screen_name=user,\n",
    "                    count=self.result_limit,\n",
    "                    tweet_mode = 'extended',\n",
    "                    include_retweets=mine_rewteets\n",
    "                )\n",
    "                \n",
    "            for item in statuses:\n",
    "                mined = {\n",
    "                    'tweet_id':        item.id,\n",
    "                    'name':            item.user.name,\n",
    "                    'screen_name':     item.user.screen_name,\n",
    "                    'retweet_count':   item.retweet_count,\n",
    "                    'text':            item.full_text,\n",
    "                    'mined_at':        datetime.datetime.now(),\n",
    "                    'created_at':      item.created_at,\n",
    "                    'favourite_count': item.favorite_count,\n",
    "                    'hashtags':        item.entities['hashtags'],\n",
    "                    'status_count':    item.user.statuses_count,\n",
    "                    'location':        item.place,\n",
    "                    'source_device':   item.source\n",
    "                }\n",
    "                try:\n",
    "                    mined['retweet_text'] = item.retweeted_status.full_text\n",
    "                except:\n",
    "                    mined['retweet_text'] = 'None'\n",
    "                try:\n",
    "                    mined['quote_text'] = item.quoted_status.full_text\n",
    "                    mined['quote_screen_name'] = status.quoted_status.user.screen_name\n",
    "                except:\n",
    "                    mined['quote_text'] = 'None'\n",
    "                    mined['quote_screen_name'] = 'None'\n",
    "                last_tweet_id = item.id\n",
    "                data.append(mined)\n",
    "            page += 1\n",
    "            \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner=TweetMiner(result_limit = 200)\n",
    "mined_tweets = miner.mine_user_tweets(user='casaleantonio', max_pages=20)\n",
    "mined_tweets_df= pd.DataFrame(mined_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mined_tweets_df.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crear un vector de palabras únicas:**\n",
    "\n",
    "Pueden usar ```set(['p1','p2','p3',...])```\n",
    "\n",
    "<mark>Nota 1:</mark> pueden hacer un prefiltrado usando lo que se conoce como **stopwords** (más información acceda [aquí](https://www.ranks.nl/stopwords)) y la herramienta para hacerlo es [NLTK (Natural Language ToolKit)](https://github.com/nltk/nltk/tree/develop/nltk). Aquí les dejo el [link!](https://stackoverflow.com/questions/5541745/get-rid-of-stopwords-and-punctuation)\n",
    "\n",
    "<mark>Nota 2:</mark> para los histogramas usen ```plt.hist``` o ```sns.distplot``` el que sea de su preferencia. Algo interesante es aprender a usar las nubes de palabras. Aquí les dejo un [ejemplo interesante de GOT](https://kaparker.com/posts/creating-word-clouds-with-python) ([otra opción más sofisticada](https://www.datacamp.com/community/tutorials/wordcloud-python)) y la [¡librería!](https://github.com/amueller/word_cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo de FIFA\n",
    "\n",
    "Se descarga un archivo comprimizo y se puede descomprimir como `unzip fifa-20-complete-player-dataset.zip` que tiene los siguientes contenidos\n",
    "```bash\n",
    "players_15.csv\n",
    "players_16.csv\n",
    "players_17.csv\n",
    "players_18.csv\n",
    "players_19.csv\n",
    "players_20.csv\n",
    "```\n",
    "Cada archivo de texto tiene el mismo número de columnas y contiene información interesante. Los invito a que lo abran (Microsoft Excel, LibreOffice, o el de su preferencia) Si no quieren hacerlo, pueden mirar las columnas con el siguiente comando: e.g. `head -n1 players15.csv | sed 's/,/\\n/g'` con el siguiente output:\n",
    "```bash\n",
    "sofifa_id\n",
    "player_url\n",
    "short_name\n",
    "long_name\n",
    "age\n",
    "dob\n",
    "height_cm\n",
    "weight_kg\n",
    "nationality\n",
    "club\n",
    "overall\n",
    "potential\n",
    "value_eur\n",
    "wage_eur\n",
    "player_positions\n",
    "preferred_foot\n",
    "international_reputation\n",
    "weak_foot\n",
    "skill_moves\n",
    "work_rate\n",
    "body_type\n",
    "real_face\n",
    "release_clause_eur\n",
    "player_tags\n",
    "team_position\n",
    "team_jersey_number\n",
    "loaned_from\n",
    "joined\n",
    "contract_valid_until\n",
    "nation_position\n",
    "nation_jersey_number\n",
    "pace\n",
    "shooting\n",
    "passing\n",
    "dribbling\n",
    "defending\n",
    "physic\n",
    "gk_diving\n",
    "gk_handling\n",
    "gk_kicking\n",
    "gk_reflexes\n",
    "gk_speed\n",
    "gk_positioning\n",
    "player_traits\n",
    "attacking_crossing\n",
    "attacking_finishing\n",
    "attacking_heading_accuracy\n",
    "attacking_short_passing\n",
    "attacking_volleys\n",
    "skill_dribbling\n",
    "skill_curve\n",
    "skill_fk_accuracy\n",
    "skill_long_passing\n",
    "skill_ball_control\n",
    "movement_acceleration\n",
    "movement_sprint_speed\n",
    "movement_agility\n",
    "movement_reactions\n",
    "movement_balance\n",
    "power_shot_power\n",
    "power_jumping\n",
    "power_stamina\n",
    "power_strength\n",
    "power_long_shots\n",
    "mentality_aggression\n",
    "mentality_interceptions\n",
    "mentality_positioning\n",
    "mentality_vision\n",
    "mentality_penalties\n",
    "mentality_composure\n",
    "defending_marking\n",
    "defending_standing_tackle\n",
    "defending_sliding_tackle\n",
    "goalkeeping_diving\n",
    "goalkeeping_handling\n",
    "goalkeeping_kicking\n",
    "goalkeeping_positioning\n",
    "goalkeeping_reflexes\n",
    "ls\n",
    "st\n",
    "rs\n",
    "lw\n",
    "lf\n",
    "cf\n",
    "rf\n",
    "rw\n",
    "lam\n",
    "cam\n",
    "ram\n",
    "lm\n",
    "lcm\n",
    "cm\n",
    "rcm\n",
    "rm\n",
    "lwb\n",
    "ldm\n",
    "cdm\n",
    "rdm\n",
    "rwb\n",
    "lb\n",
    "lcb\n",
    "cb\n",
    "rcb\n",
    "rb\n",
    "```\n",
    "\n",
    "Para el ejercicio nos interesan: <u>nationality</u> y <u>overall</u> (¿quizás <u>potential</u>?)\n",
    "Para cargar el archivo, por ejemplo `players_15.csv` y sólo esas columnas podemos usar el comando de Numpy [loadtxt](https://docs.scipy.org/doc/numpy/reference/generated/numpy.loadtxt.html) o [genfromtxt](https://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html). Aquí les dejo el dos scripts de ejemplo para cada comando:\n",
    "```python\n",
    "test1 = np.loadtxt(\n",
    "    'players_15.csv', skiprows=1, usecols=(8,10,11), delimiter=',',\n",
    "    dtype={\n",
    "        'names': ('nationality', 'overall', 'potential'),\n",
    "        'formats': ('S20', np.int8, np.int8)\n",
    "    }\n",
    ")\n",
    "print(test1)\n",
    "\n",
    "test2 = np.genfromtxt(\n",
    "    'players_15.csv', skip_header=True, usecols=(8,10,11), delimiter=',',\n",
    "    dtype=None, names=('nationality', 'overall', 'potential'),\n",
    "    encoding=None\n",
    ")\n",
    "print(test2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fisi2028]",
   "language": "python",
   "name": "conda-env-fisi2028-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
